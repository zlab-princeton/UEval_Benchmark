1:"$Sreact.fragment"
2:I[60729,["2619","static/chunks/2619-6e4b0f7a5e306a67.js","8720","static/chunks/8720-84a9ef373a7f70f3.js","3131","static/chunks/3131-b09b0f545be46bfd.js","7177","static/chunks/app/layout-1cb9fbcc639ea8d3.js"],"RootProvider"]
3:I[76454,["2619","static/chunks/2619-6e4b0f7a5e306a67.js","8720","static/chunks/8720-84a9ef373a7f70f3.js","3131","static/chunks/3131-b09b0f545be46bfd.js","7177","static/chunks/app/layout-1cb9fbcc639ea8d3.js"],"NuqsAdapter"]
4:I[9766,[],""]
5:I[49567,["5707","static/chunks/5707-3ed4755c3efe5d6d.js","8039","static/chunks/app/error-ae7bc8cfe55bb9bb.js"],"default"]
6:I[98924,[],""]
7:I[70240,["2619","static/chunks/2619-6e4b0f7a5e306a67.js","8720","static/chunks/8720-84a9ef373a7f70f3.js","3131","static/chunks/3131-b09b0f545be46bfd.js","7177","static/chunks/app/layout-1cb9fbcc639ea8d3.js"],"Toaster"]
8:I[67652,["5707","static/chunks/5707-3ed4755c3efe5d6d.js","2906","static/chunks/2906-d3accac85286ef3a.js","4127","static/chunks/4127-d7e3f6c805caa2e3.js","7712","static/chunks/7712-4cbad2349309b180.js","2659","static/chunks/2659-581a7e9225987806.js","2814","static/chunks/2814-7a47901a8df8142e.js","4790","static/chunks/app/(home)/layout-5a237f94e77629d3.js"],"NavProvider"]
9:I[32465,["5707","static/chunks/5707-3ed4755c3efe5d6d.js","2906","static/chunks/2906-d3accac85286ef3a.js","4127","static/chunks/4127-d7e3f6c805caa2e3.js","7712","static/chunks/7712-4cbad2349309b180.js","2659","static/chunks/2659-581a7e9225987806.js","2814","static/chunks/2814-7a47901a8df8142e.js","4790","static/chunks/app/(home)/layout-5a237f94e77629d3.js"],"Navbar"]
a:I[1481,["5707","static/chunks/5707-3ed4755c3efe5d6d.js","2619","static/chunks/2619-6e4b0f7a5e306a67.js","2906","static/chunks/2906-d3accac85286ef3a.js","4127","static/chunks/4127-d7e3f6c805caa2e3.js","7712","static/chunks/7712-4cbad2349309b180.js","2659","static/chunks/2659-581a7e9225987806.js","5965","static/chunks/5965-186e4c3e189a57da.js","8720","static/chunks/8720-84a9ef373a7f70f3.js","5580","static/chunks/5580-c6313e80a056744b.js","9065","static/chunks/9065-79c6eac424f842ad.js","4522","static/chunks/4522-a272cb4d9f4960d9.js","9326","static/chunks/9326-99951e5e03ca56cd.js","1387","static/chunks/app/(home)/news/%5Bslug%5D/page-306decc06d173de2.js"],"default"]
b:I[32465,["5707","static/chunks/5707-3ed4755c3efe5d6d.js","2906","static/chunks/2906-d3accac85286ef3a.js","4127","static/chunks/4127-d7e3f6c805caa2e3.js","7712","static/chunks/7712-4cbad2349309b180.js","2659","static/chunks/2659-581a7e9225987806.js","2814","static/chunks/2814-7a47901a8df8142e.js","4790","static/chunks/app/(home)/layout-5a237f94e77629d3.js"],"NavbarLink"]
c:I[49557,["5707","static/chunks/5707-3ed4755c3efe5d6d.js","2906","static/chunks/2906-d3accac85286ef3a.js","4127","static/chunks/4127-d7e3f6c805caa2e3.js","7712","static/chunks/7712-4cbad2349309b180.js","2659","static/chunks/2659-581a7e9225987806.js","2814","static/chunks/2814-7a47901a8df8142e.js","4790","static/chunks/app/(home)/layout-5a237f94e77629d3.js"],"ThemeToggle"]
d:I[20356,["5707","static/chunks/5707-3ed4755c3efe5d6d.js","2906","static/chunks/2906-d3accac85286ef3a.js","4127","static/chunks/4127-d7e3f6c805caa2e3.js","7712","static/chunks/7712-4cbad2349309b180.js","2659","static/chunks/2659-581a7e9225987806.js","2814","static/chunks/2814-7a47901a8df8142e.js","4790","static/chunks/app/(home)/layout-5a237f94e77629d3.js"],"Menu"]
e:I[20356,["5707","static/chunks/5707-3ed4755c3efe5d6d.js","2906","static/chunks/2906-d3accac85286ef3a.js","4127","static/chunks/4127-d7e3f6c805caa2e3.js","7712","static/chunks/7712-4cbad2349309b180.js","2659","static/chunks/2659-581a7e9225987806.js","2814","static/chunks/2814-7a47901a8df8142e.js","4790","static/chunks/app/(home)/layout-5a237f94e77629d3.js"],"MenuTrigger"]
f:I[20356,["5707","static/chunks/5707-3ed4755c3efe5d6d.js","2906","static/chunks/2906-d3accac85286ef3a.js","4127","static/chunks/4127-d7e3f6c805caa2e3.js","7712","static/chunks/7712-4cbad2349309b180.js","2659","static/chunks/2659-581a7e9225987806.js","2814","static/chunks/2814-7a47901a8df8142e.js","4790","static/chunks/app/(home)/layout-5a237f94e77629d3.js"],"MenuContent"]
10:I[20356,["5707","static/chunks/5707-3ed4755c3efe5d6d.js","2906","static/chunks/2906-d3accac85286ef3a.js","4127","static/chunks/4127-d7e3f6c805caa2e3.js","7712","static/chunks/7712-4cbad2349309b180.js","2659","static/chunks/2659-581a7e9225987806.js","2814","static/chunks/2814-7a47901a8df8142e.js","4790","static/chunks/app/(home)/layout-5a237f94e77629d3.js"],"MenuLinkItem"]
17:I[57150,[],""]
:HL["/UEval/_next/static/media/66f30814ff6d7cdf.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/UEval/_next/static/media/e4af272ccee01ff0-s.p.woff2","font",{"crossOrigin":"","type":"font/woff2"}]
:HL["/UEval/_next/static/css/5a9dae0d910ab8e0.css","style"]
:HL["/UEval/_next/static/css/f0b4f6c31c32f4c0.css","style"]
0:{"P":null,"b":"cJ2AMD_FyXA9wZjgwrnSX","p":"/UEval","c":["","news","registry-and-adapters"],"i":false,"f":[[["",{"children":["(home)",{"children":["news",{"children":[["slug","registry-and-adapters","d"],{"children":["__PAGE__",{}]}]}]}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/UEval/_next/static/css/5a9dae0d910ab8e0.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"__className_370cd6 __variable_3b6218","suppressHydrationWarning":true,"children":["$","body",null,{"className":"flex min-h-screen flex-col","children":[["$","$L2",null,{"children":["$","$L3",null,{"children":["$","$L4",null,{"parallelRouterKey":"children","error":"$5","errorStyles":[],"errorScripts":[],"template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}]}],["$","$L7",null,{}]]}]}]]}],{"children":["(home)",["$","$1","c",{"children":[null,["$","$L8",null,{"transparentMode":"$undefined","children":["$","main",null,{"id":"nd-home-layout","children":[["$","$L9",null,{"children":[["$","$La",null,{"href":"/","className":"inline-flex items-center gap-2.5 font-semibold","children":["$","div",null,{"className":"flex items-center gap-2","children":[["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide lucide-image size-4","aria-hidden":"true","children":[["$","rect","1m3agn",{"width":"18","height":"18","x":"3","y":"3","rx":"2","ry":"2"}],["$","circle","af1f0g",{"cx":"9","cy":"9","r":"2"}],["$","path","1xmnt7",{"d":"m21 15-3.086-3.086a2 2 0 0 0-2.828 0L6 21"}],"$undefined"]}],["$","p",null,{"className":"font-mono text-base font-medium tracking-tight","children":"UEval"}]]}]}],"$undefined",["$","ul",null,{"className":"flex flex-row items-center gap-2 px-6 max-sm:hidden","children":[["$","$Lb","0",{"className":"text-sm","item":{"text":"Leaderboard","url":"/leaderboard","active":"nested-url"},"variant":"$undefined","aria-label":"$undefined","children":"Leaderboard"}]]}],["$","div",null,{"className":"flex flex-row items-center justify-end gap-1.5 flex-1","children":[false,["$","$Lc",null,{"className":"max-lg:hidden","mode":"light-dark-system"}],null]}],["$","ul",null,{"className":"flex flex-row items-center","children":[[],["$","$Ld",null,{"className":"lg:hidden","children":[["$","$Le",null,{"aria-label":"Toggle Menu","className":"inline-flex items-center justify-center rounded-md text-sm font-medium transition-colors duration-100 disabled:pointer-events-none disabled:opacity-50 focus-visible:outline-none hover:bg-fd-accent hover:text-fd-accent-foreground p-1.5 [&_svg]:size-5 group -me-1.5","enableHover":"$undefined","children":["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide !size-5.5 transition-transform duration-300 group-data-[state=open]:rotate-180","children":[[["$","path","qrunsl",{"d":"m6 9 6 6 6-6"}]],"$undefined"]}]}],["$","$Lf",null,{"className":"sm:flex-row sm:items-center sm:justify-end","children":[[["$","$L10","0",{"item":"$0:f:0:1:2:children:1:props:children:1:props:children:props:children:0:props:children:2:props:children:0:props:item","className":"sm:hidden"}]],["$","div",null,{"className":"-ms-1.5 flex flex-row items-center gap-1.5 max-sm:mt-2","children":[[],["$","div",null,{"role":"separator","className":"flex-1"}],null,"$L11"]}]]}]]}]]}]]}],"$L12"],"className":"flex flex-1 flex-col pt-14"}]}]]}],{"children":["news","$L13",{"children":[["slug","registry-and-adapters","d"],"$L14",{"children":["__PAGE__","$L15",{},null,false]},null,false]},null,false]},null,false]},null,false],"$L16",false]],"m":"$undefined","G":["$17",[]],"s":false,"S":true}
19:I[24431,[],"OutletBoundary"]
1b:I[15278,[],"AsyncMetadataOutlet"]
1d:I[24431,[],"ViewportBoundary"]
1f:I[24431,[],"MetadataBoundary"]
20:"$Sreact.suspense"
11:["$","$Lc",null,{"mode":"light-dark-system"}]
12:["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":"$0:f:0:1:1:props:children:1:props:children:props:children:0:props:children:props:children:props:notFound:0:1:props:style","children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":"$0:f:0:1:1:props:children:1:props:children:props:children:0:props:children:props:children:props:notFound:0:1:props:children:props:children:1:props:style","children":404}],["$","div",null,{"style":"$0:f:0:1:1:props:children:1:props:children:props:children:0:props:children:props:children:props:notFound:0:1:props:children:props:children:2:props:style","children":["$","h2",null,{"style":"$0:f:0:1:1:props:children:1:props:children:props:children:0:props:children:props:children:props:notFound:0:1:props:children:props:children:2:props:children:props:style","children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]
13:["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
14:["$","$1","c",{"children":[null,["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L6",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":"$undefined","forbidden":"$undefined","unauthorized":"$undefined"}]]}]
15:["$","$1","c",{"children":["$L18",[["$","link","0",{"rel":"stylesheet","href":"/UEval/_next/static/css/f0b4f6c31c32f4c0.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","$L19",null,{"children":["$L1a",["$","$L1b",null,{"promise":"$@1c"}]]}]]}]
16:["$","$1","h",{"children":[null,[["$","$L1d",null,{"children":"$L1e"}],["$","meta",null,{"name":"next-size-adjust","content":""}]],["$","$L1f",null,{"children":["$","div",null,{"hidden":true,"children":["$","$20",null,{"fallback":null,"children":"$L21"}]}]}]]}]
22:I[7656,["5707","static/chunks/5707-3ed4755c3efe5d6d.js","2619","static/chunks/2619-6e4b0f7a5e306a67.js","2906","static/chunks/2906-d3accac85286ef3a.js","4127","static/chunks/4127-d7e3f6c805caa2e3.js","7712","static/chunks/7712-4cbad2349309b180.js","2659","static/chunks/2659-581a7e9225987806.js","5965","static/chunks/5965-186e4c3e189a57da.js","8720","static/chunks/8720-84a9ef373a7f70f3.js","5580","static/chunks/5580-c6313e80a056744b.js","9065","static/chunks/9065-79c6eac424f842ad.js","4522","static/chunks/4522-a272cb4d9f4960d9.js","9326","static/chunks/9326-99951e5e03ca56cd.js","1387","static/chunks/app/(home)/news/%5Bslug%5D/page-306decc06d173de2.js"],"Share"]
18:["$","div",null,{"className":"flex flex-1 flex-col items-center px-4","children":["$","div",null,{"className":"flex w-full max-w-4xl flex-1 flex-col","children":[["$","div",null,{"className":"flex-1 pt-6 sm:pt-12","children":[["$","div",null,{"className":"mb-6 flex items-center justify-between gap-2","children":[["$","p",null,{"className":"text-fd-muted-foreground font-mono text-sm","children":["Tue Jul 15 2025"," • ","Release"]}],["$","$L22",null,{}]]}],["$","h1",null,{"className":"mb-8 font-mono text-4xl/normal font-medium tracking-tight","children":"Introducing the Terminal-Bench Dataset Registry with SWE-Bench Verified, AppWorld, DevEval, and EvoEval"}],["$","p",null,{"className":"text-fd-muted-foreground font-mono","children":"An easy way to evaluate agents on popular benchmarks and distribute new benchmarks to agent developers."}],false]}],["$","article",null,{"className":"flex w-full flex-col py-8","children":[["$","div",null,{"className":"prose min-w-0","children":[["$","p",null,{"children":"Today we’re announcing the Terminal-Bench registry: an easy way to evaluate agents on many agentic benchmarks via the Terminal-Bench framework. For benchmark developers, the Terminal-Bench registry also offers a way to distribute benchmarks to agent developers and run existing agents in a unified way."}],"\n",["$","p",null,{"children":"Terminal-Bench-Core, our benchmark for evaluating agents in the command line, provides a comprehensive testbed for measuring agents on software engineering, scientific computing, system administration, and more. But benchmarks from other researchers, like SWE-Bench, AppWorld, DevEval, etc. help paint a broader picture of agent performance on specific domains."}],"\n",["$","p",null,{"children":"Until now, evaluating agents on multiple benchmarks has been difficult and time-consuming. Each benchmark comes with its own repository, evaluation harness, dependencies, and Docker images. Testing a new agent or model means cloning each of these repositories and spending hours or days familiarizing yourself with the project's structure and getting it to run."}],"\n",["$","p",null,{"children":"The Terminal-Bench registry solves this problem. Benchmark developers can now build new benchmarks using—or adapt existing ones to use—the Terminal-Bench harness, which provides logging, live streaming, popular agent integrations, parallelization, distribution via Terminal-Bench CLI, and, coming soon, cloud hosting out of the box."}],"\n",["$","p",null,{"children":"To supplement the launch of the Terminal-Bench registry, we’ve adapted four popular benchmarks into the Terminal-Bench framework and registered them on the Terminal-Bench registry."}],"\n",["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Benchmark"}],["$","th",null,{"children":"Description"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":["$","$La",null,{"href":"https://www.swebench.com/","children":["$","strong",null,{"children":"SWE-Bench Verified"}]}]}],["$","td",null,{"children":"A well-known benchmark for evaluating LLMs on real-world software issues collected from GitHub. Given a codebase and an issue, a language model is tasked with generating a patch that resolves the described problem."}]]}],["$","tr",null,{"children":[["$","td",null,{"children":["$","$La",null,{"href":"https://appworld.dev/","children":["$","strong",null,{"children":"AppWorld"}]}]}],["$","td",null,{"children":"A suite of natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It focuses on practical, multi-step operations that require interaction with various tools and APIs to achieve a goal."}]]}],"$L23","$L24"]}]]}]}],"\n","$L25","\n","$L26","\n","$L27","\n","$L28","\n","$L29","\n","$L2a","\n","\n","$L2b","\n","$L2c","\n","$L2d","\n","$L2e","\n","$L2f","\n","$L30","\n","$L31","\n","$L32","\n","$L33","\n","$L34","\n","$L35","\n","$L36","\n","$L37","\n","$L38","\n","$L39","\n","$L3a","\n","$L3b","\n","$L3c","\n","$L3d","\n","$L3e"]}],"$L3f"]}]]}]}]
40:I[75580,["5707","static/chunks/5707-3ed4755c3efe5d6d.js","2619","static/chunks/2619-6e4b0f7a5e306a67.js","2906","static/chunks/2906-d3accac85286ef3a.js","4127","static/chunks/4127-d7e3f6c805caa2e3.js","7712","static/chunks/7712-4cbad2349309b180.js","2659","static/chunks/2659-581a7e9225987806.js","5965","static/chunks/5965-186e4c3e189a57da.js","8720","static/chunks/8720-84a9ef373a7f70f3.js","5580","static/chunks/5580-c6313e80a056744b.js","9065","static/chunks/9065-79c6eac424f842ad.js","4522","static/chunks/4522-a272cb4d9f4960d9.js","9326","static/chunks/9326-99951e5e03ca56cd.js","1387","static/chunks/app/(home)/news/%5Bslug%5D/page-306decc06d173de2.js"],"CodeBlock"]
41:I[75580,["5707","static/chunks/5707-3ed4755c3efe5d6d.js","2619","static/chunks/2619-6e4b0f7a5e306a67.js","2906","static/chunks/2906-d3accac85286ef3a.js","4127","static/chunks/4127-d7e3f6c805caa2e3.js","7712","static/chunks/7712-4cbad2349309b180.js","2659","static/chunks/2659-581a7e9225987806.js","5965","static/chunks/5965-186e4c3e189a57da.js","8720","static/chunks/8720-84a9ef373a7f70f3.js","5580","static/chunks/5580-c6313e80a056744b.js","9065","static/chunks/9065-79c6eac424f842ad.js","4522","static/chunks/4522-a272cb4d9f4960d9.js","9326","static/chunks/9326-99951e5e03ca56cd.js","1387","static/chunks/app/(home)/news/%5Bslug%5D/page-306decc06d173de2.js"],"Pre"]
42:I[40149,["5707","static/chunks/5707-3ed4755c3efe5d6d.js","2619","static/chunks/2619-6e4b0f7a5e306a67.js","2906","static/chunks/2906-d3accac85286ef3a.js","4127","static/chunks/4127-d7e3f6c805caa2e3.js","7712","static/chunks/7712-4cbad2349309b180.js","2659","static/chunks/2659-581a7e9225987806.js","5965","static/chunks/5965-186e4c3e189a57da.js","8720","static/chunks/8720-84a9ef373a7f70f3.js","5580","static/chunks/5580-c6313e80a056744b.js","9065","static/chunks/9065-79c6eac424f842ad.js","4522","static/chunks/4522-a272cb4d9f4960d9.js","9326","static/chunks/9326-99951e5e03ca56cd.js","1387","static/chunks/app/(home)/news/%5Bslug%5D/page-306decc06d173de2.js"],"default"]
23:["$","tr",null,{"children":[["$","td",null,{"children":["$","$La",null,{"href":"https://arxiv.org/abs/2405.19856","children":["$","strong",null,{"children":"DevEval"}]}]}],["$","td",null,{"children":"A benchmark designed to evaluate LLMs across various stages of the repo-level software development lifecycle, including software design, environment setup, implementation, acceptance testing, and unit testing."}]]}]
24:["$","tr",null,{"children":[["$","td",null,{"children":["$","$La",null,{"href":"https://evo-eval.github.io/","children":["$","strong",null,{"children":"EvoEval"}]}]}],["$","td",null,{"children":"\"A program synthesis benchmark suite created by evolving existing benchmarks into different targeted domains for a comprehensive evaluation of LLM coding abilities.\""}]]}]
25:["$","p",null,{"children":"The Terminal-Bench registry enables agent developers to integrate their agent into Terminal-Bench once and immediately start evaluating their agent on registered evals or even start building their own set of domain-specific in-house evals."}]
26:["$","p",null,{"children":"To view the dataset registry, run"}]
27:["$","$L40",null,{"className":"shiki shiki-themes github-light github-dark","style":{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},"tabIndex":"0","icon":"<svg viewBox=\"0 0 24 24\"><path d=\"m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z\" fill=\"currentColor\" /></svg>","children":["$","$L41",null,{"children":["$","code",null,{"children":["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},"children":"tb"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" datasets"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" list"}],["$","span",null,{"style":{"--shiki-light":"#6A737D","--shiki-dark":"#6A737D"},"children":" # Make sure to pip install terminal-bench first"}]]}]}]}]}]
28:["$","p",null,{"children":["Then, for example, to run ",["$","code",null,{"children":"django__django-13658"}]," from SWE-Bench Verified with Terminus and Claude 4 Sonnet, run"]}]
29:["$","$L40",null,{"className":"shiki shiki-themes github-light github-dark","style":{"--shiki-light":"#24292e","--shiki-dark":"#e1e4e8","--shiki-light-bg":"#fff","--shiki-dark-bg":"#24292e"},"tabIndex":"0","icon":"<svg viewBox=\"0 0 24 24\"><path d=\"m 4,4 a 1,1 0 0 0 -0.7070312,0.2929687 1,1 0 0 0 0,1.4140625 L 8.5859375,11 3.2929688,16.292969 a 1,1 0 0 0 0,1.414062 1,1 0 0 0 1.4140624,0 l 5.9999998,-6 a 1.0001,1.0001 0 0 0 0,-1.414062 L 4.7070312,4.2929687 A 1,1 0 0 0 4,4 Z m 8,14 a 1,1 0 0 0 -1,1 1,1 0 0 0 1,1 h 8 a 1,1 0 0 0 1,-1 1,1 0 0 0 -1,-1 z\" fill=\"currentColor\" /></svg>","children":["$","$L41",null,{"children":["$","code",null,{"children":[["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#6F42C1","--shiki-dark":"#B392F0"},"children":"tb"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" run"}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":" \\\\"}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"  --dataset"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" swebench-verified"}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":" \\\\"}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"  --task-id"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" django__django-13658"}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":" \\\\"}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"  --agent"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" terminus"}],["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":" \\\\"}]]}],"\n",["$","span",null,{"className":"line","children":[["$","span",null,{"style":{"--shiki-light":"#005CC5","--shiki-dark":"#79B8FF"},"children":"  --model"}],["$","span",null,{"style":{"--shiki-light":"#032F62","--shiki-dark":"#9ECBFF"},"children":" anthropic/claude-sonnet-4-20250514"}],["$","span",null,{"style":{"--shiki-light":"#24292E","--shiki-dark":"#E1E4E8"},"children":"  "}]]}]]}]}]}]
2a:["$","p",null,{"children":"which produces the following run."}]
2b:["$","$L42",null,{"src":"/django__django-13658.cast","theme":"asciinema","terminalFontFamily":"var(--font-geist-mono)","autoPlay":true,"loop":false}]
2c:["$","p",null,{"children":"We plan to regularly add more adapters for frontier benchmarks (suggestions are welcome!) and encourage the community to take advantage of the registry to develop and distribute new benchmarks."}]
2d:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"validating-benchmark-adaptations","children":[["$","a",null,{"data-card":"","href":"#validating-benchmark-adaptations","className":"peer","children":"Validating Benchmark Adaptations"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
2e:["$","p",null,{"children":"Terminal-Bench adapters restructure existing benchmarks to use the Terminal-Bench task framework but do not alter the task contents. From the agent's perspective, a task in the adapted benchmark is the same instruction and environment and completing the task is determined by the same set of unit tests."}]
2f:["$","p",null,{"children":"To ensure each benchmark has been adapted correctly, we run parity experiments using the benchmark's harness and the Terminal-Bench harness with the same agent, prompt, and model."}]
30:["$","p",null,{"children":"Terminal-Bench adapter resolved rates are comparable to those of the original benchmarks:"}]
31:["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Agent"}],["$","th",null,{"children":"Model"}],["$","th",null,{"children":"Original SWE-Bench Grading Script*"}],["$","th",null,{"children":"Terminal-Bench Adapter"}]]}]}],["$","tbody",null,{"children":["$","tr",null,{"children":[["$","td",null,{"children":"Terminus"}],["$","td",null,{"children":["$","code",null,{"children":"claude-4-opus"}]}],["$","td",null,{"children":"66%"}],["$","td",null,{"children":"66%"}]]}]}]]}]}]
32:["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Agent"}],["$","th",null,{"children":"Model"}],["$","th",null,{"children":"Original AppWorld**"}],["$","th",null,{"children":"Terminal-Bench Adapter"}]]}]}],["$","tbody",null,{"children":["$","tr",null,{"children":[["$","td",null,{"children":"Claude Code"}],["$","td",null,{"children":["$","code",null,{"children":"claude-4-opus"}]}],["$","td",null,{"children":"52.1% ± 1.8%"}],["$","td",null,{"children":"52.1% ± 1.8%"}]]}]}]]}]}]
33:["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Agent"}],["$","th",null,{"children":"Model"}],["$","th",null,{"children":"Original DevEval"}],["$","th",null,{"children":"Terminal-Bench Adapter"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"Claude Code"}],["$","td",null,{"children":["$","code",null,{"children":"claude-4-opus"}]}],["$","td",null,{"children":"22.8% ± 4.3%"}],["$","td",null,{"children":"21.6% ± 3.3%"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Codex"}],["$","td",null,{"children":["$","code",null,{"children":"o4-mini"}]}],["$","td",null,{"children":"22.4% ± 4.4%"}],["$","td",null,{"children":"23.6% ± 2.3%"}]]}]]}]]}]}]
34:["$","div",null,{"className":"relative overflow-auto prose-no-margin my-6","children":["$","table",null,{"children":[["$","thead",null,{"children":["$","tr",null,{"children":[["$","th",null,{"children":"Agent"}],["$","th",null,{"children":"Model"}],["$","th",null,{"children":"Original EvoEval"}],["$","th",null,{"children":"Terminal-Bench Adapter"}]]}]}],["$","tbody",null,{"children":[["$","tr",null,{"children":[["$","td",null,{"children":"Claude Code"}],["$","td",null,{"children":["$","code",null,{"children":"claude-4-opus"}]}],["$","td",null,{"children":"65.8% ± 1.7%"}],["$","td",null,{"children":"66.4% ± 1.4%"}]]}],["$","tr",null,{"children":[["$","td",null,{"children":"Codex"}],["$","td",null,{"children":["$","code",null,{"children":"o4-mini"}]}],["$","td",null,{"children":"67.1% ± 1.1%"}],["$","td",null,{"children":"66.4% ± 2.0%"}]]}]]}]]}]}]
35:["$","p",null,{"children":"Terminal-Bench will continue to require parity experiments for adapted benchmarks so agent developers can evaluate on the benchmarks using the Terminal-Bench harness with full confidence."}]
36:["$","p",null,{"children":"*Note that when SWE-bench was released, the task was to generate a patch file given context from the repo in a single model call. There is no first-party evaluation harness, but rather a script to grade patch files. To ensure our adaptation is correct, we ran Terminus using the Terminal-Bench harness, extracted the patches, and evaluated them using the SWE-bench grading script, which produced the same results as our harness."}]
37:["$","p",null,{"children":"** Post-publication AppWorld released a CLI for terminal-based agents to interact with the environment. With the support of the AppWorld authors, we extracted agent actions on this CLI from our harness runs and evaluated them using the original AppWorld grading script, which produced the same results as our harness."}]
38:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"for-benchmark-developers","children":[["$","a",null,{"data-card":"","href":"#for-benchmark-developers","className":"peer","children":"For Benchmark Developers"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
39:["$","p",null,{"children":"The Terminal-Bench framework supports text-based agent benchmarks that can be represented as an instruction, docker environment, and test script."}]
3a:["$","p",null,{"children":["Interested in making it easier for users to run your benchmark? Consider ",["$","$La",null,{"href":"/docs/adapters","children":"building an adapter"}],"."]}]
3b:["$","p",null,{"children":["Building a new benchmark? Use the Terminal-Bench framework to build (",["$","code",null,{"children":"tb tasks create"}],"), quality check (",["$","code",null,{"children":"tb tasks check"}],"), run (",["$","code",null,{"children":"tb run"}],"), and register your benchmark to take advantage of the harness and logging infra so you can focus on building tasks and running experiments."]}]
3c:["$","h2",null,{"className":"flex scroll-m-28 flex-row items-center gap-2","id":"planned-adapters","children":[["$","a",null,{"data-card":"","href":"#planned-adapters","className":"peer","children":"Planned Adapters"}],["$","svg",null,{"ref":"$undefined","xmlns":"http://www.w3.org/2000/svg","width":24,"height":24,"viewBox":"0 0 24 24","fill":"none","stroke":"currentColor","strokeWidth":2,"strokeLinecap":"round","strokeLinejoin":"round","className":"lucide size-3.5 shrink-0 text-fd-muted-foreground opacity-0 transition-opacity peer-hover:opacity-100","aria-label":"Link to section","children":[[["$","path","1cjeqo",{"d":"M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"}],["$","path","19qd67",{"d":"M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"}]],"$undefined"]}]]}]
3d:["$","p",null,{"children":"We plan to adapt the following benchmarks into the Terminal-Bench framework: MLE-Bench, SWE-Lancer, RE-Bench, BIX-Bench, The Agent Company, Research Bench, Cybench, and AlgoTune."}]
3e:["$","p",null,{"children":["If you have a Terminal-Bench compatible benchmark you'd like to see adapted, please ",["$","$La",null,{"href":"https://github.com/laude-institute/terminal-bench/issues/new","children":"open an issue"}]," or let us know on ",["$","$La",null,{"href":"https://discord.gg/6xWPKhGDbA","children":"Discord"}],"."]}]
3f:["$","div",null,{"className":"mt-12 flex flex-col gap-4 text-sm","children":["$","div",null,{"children":[["$","p",null,{"className":"text-fd-muted-foreground mb-1 font-mono","children":"Written by"}],["$","p",null,{"className":"font-mono","children":[["$","span","The Terminal-Bench Team",{"children":[["$","a",null,{"href":"$undefined","className":"underline-offset-4 hover:underline","children":"The Terminal-Bench Team"}],false]}]]}]]}]}]
1e:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
1a:null
43:I[80622,[],"IconMark"]
1c:{"metadata":[["$","title","0",{"children":"Introducing the Terminal-Bench Dataset Registry with SWE-Bench Verified, AppWorld, DevEval, and EvoEval"}],["$","meta","1",{"name":"description","content":"An easy way to evaluate agents on popular benchmarks and distribute new benchmarks to agent developers."}],["$","meta","2",{"property":"og:title","content":"UEval: A Benchmark for Unified Multimodal Generation"}],["$","meta","3",{"property":"og:description","content":"UEval is a challenging real-world benchmark for multimodal generation of unified models that are capable of generating both images and text."}],["$","meta","4",{"property":"og:url","content":"http://localhost:3000/UEval"}],["$","meta","5",{"property":"og:site_name","content":"UEval"}],["$","meta","6",{"property":"og:locale","content":"en_US"}],["$","meta","7",{"property":"og:image","content":"http://localhost:3000/UEval/UEval/og.png"}],["$","meta","8",{"property":"og:type","content":"website"}],["$","meta","9",{"name":"twitter:card","content":"summary_large_image"}],["$","meta","10",{"name":"twitter:title","content":"UEval: A Benchmark for Unified Multimodal Generation"}],["$","meta","11",{"name":"twitter:description","content":"UEval is a challenging real-world benchmark for multimodal generation of unified models that are capable of generating both images and text."}],["$","meta","12",{"name":"twitter:image","content":"http://localhost:3000/UEval/UEval/og.png"}],["$","meta","13",{"name":"twitter:image:width","content":"1200"}],["$","meta","14",{"name":"twitter:image:height","content":"630"}],["$","link","15",{"rel":"icon","href":"/favicon.ico"}],["$","$L43","16",{}]],"error":null,"digest":"$undefined"}
21:"$1c:metadata"
